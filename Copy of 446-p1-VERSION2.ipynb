{"cells":[{"cell_type":"markdown","metadata":{"id":"ZHj92VDfTEPO"},"source":["Your name: Quang Hung Nguyen\n","\n","Your student ID number: 33855953\n","\n","Shared link to this notebook:https://colab.research.google.com/drive/1C-SMhLjiKVqhdUVEpj9XGvqMUbNzRjBJ?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"HoFerkqpTEPQ"},"source":["## Programming Assignment 1 (P1) for COMPSCI 446, Search Engines"]},{"cell_type":"markdown","metadata":{"id":"sVSv_Zs7TEPR"},"source":["**WARNING**: This is a preliminary version of the P1 project. Whatever you do on this version will need to be copied over into the final version of the project. We hope that it will carry over easily.\n","\n","The purpose of this project is to explore tokenization, stopword removal, and stemming within a few real documents, and to investigate term statistics as related to Heaps' and possibly Zipf's Laws. Although actual time to complete this project will vary widely across students, expect to spend several hours if you are a strong programmer and 10+ hours if you're a bit rusty.\n","\n","The description below is quite lengthy and will probably feel complicated at first, but each portion is just one piece of sequence of rules that you need to apply to the tokens in the file. At a high level, the program will do the following:\n","* It will read in the (gzip compressed) input file and break it into tokens (loosely speaking, \"words\") using spaces or a fancier approach.\n","* It will optionally remove tokens that match stopwords in a provided list.\n","* It will optionally stem those tokens to their root form using a simple suffix-s stemmer or the Porter stemmer.\n","* It will calculate some statistics and summary information about the resulting tokens.\n","* It will generate incremental and cumulative information about the tokens the system encounters and generates."]},{"cell_type":"markdown","metadata":{"id":"EQ5lzWGuTEPS"},"source":["Here are a list of provided files (**these are provided as described but may change for the final project**) that will be loaded into your Google Drive for use in this notebook:\n","\n","* _P1-train.gz_, a compressed file that you can use as input to your program.\n","\n","* _PandP.gz_, which is a much larger text file for you to play with, though we will not provide any sample output. This is the text of Jane Austen's novel, _Pride and Prejudice_, downloaded from Project Gutenberg. You can see the original at https://www.gutenberg.org/ebooks/1342; what is provided below is the UTF-8 version, but with leading and trailing material removed, and with some unusual characters converted to ASCII equivalents (e.g., smart quotation marks, the British pound symbol).\n","\n","* _stopwords.txt_ is a file containing the stopwords you will use in the stopping step.\n","\n","\n","Note, after gradescope autograder is configured (**it is not ready yet**!), uploading the notebook file to gradescope will show the result on sample inputs. Please make sure the notebook is compilable (not only locally to your computer, but also on gradescope).\n","\n","Execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate drive_path value. (The autograder will not use your Google Drive.)"]},{"cell_type":"code","source":["version = 2.1 # DO NOT MODIFY. If notebook does not match with autograder version, all tests would fail!!"],"metadata":{"id":"hWAYQYm7AiFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjCDAZdyTEPT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727383191531,"user_tz":240,"elapsed":26482,"user":{"displayName":"Hung Nguyen","userId":"04521943953761589776"}},"outputId":"dec97621-0986-4681-e9c7-9baa064d1abf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["import os\n","import string\n","import gzip\n","import re\n","\n","from collections import Counter\n","\n","try:\n","    from google.colab import drive\n","    in_colab = True\n","except ImportError:\n","    in_colab = False\n","\n","\n","# You are more than welcome to code some helper functions.\n","# But do note that we are only grading functions that are coded in the template files.\n","\n","\n","# Connect to Google Drive and download copies of the sample files listed above.\n","# Please allow the access to your Google Drive or the following dataset loader will fail.\n","# (The autograder will not use your Google Drive.)\n","if in_colab:\n","  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n","  data_path = \"/content/drive/MyDrive/Colab Notebooks\" ## CHANGE TO YOUR OWN FOLDER ON GOOGLE DRIVE\n","else:\n","  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS"]},{"cell_type":"markdown","metadata":{"id":"FixeP9rOTEPT"},"source":["### 0. Pre-processing\n","\n","We will need to load the texts that stored at <em>documents_path</em> before we want to perform any kind of operation on them.\n","\n","Note, in this assignment, to practice tokenization process, we load the entire file directly into the memory and then process it line by line. However, in the real-world scenario, when dealing with large dataset (saying billions of entries), it is better to process the content line by line to save the space and memory of the machine."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKl-5NrlTEPU","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1727676385725,"user_tz":240,"elapsed":183,"user":{"displayName":"Hung Nguyen","userId":"04521943953761589776"}},"outputId":"83be73ac-5e36-46ca-e5e0-d958010aa4aa"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'os' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-114845857f40>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mstopwords_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stopwords.txt\"\u001b[0m  \u001b[0;31m# path to the file that contains stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-114845857f40>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(file_path, gz_zip)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mwebloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://cs.umass.edu/~allan/cs446/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlocal_google_drive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlocal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_google_drive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocal_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["import urllib.request\n","import gzip\n","from pathlib import Path\n","\n","def load_file(file_path: str, gz_zip: bool = True) -> list[str]:\n","    \"\"\"\n","    Load strings from the text or gzip file. Remember to strip newline if necessary!\n","\n","    Args:\n","        file_path: the location of file we want to analyze on.\n","\n","    Returns: an array of string loaded from the text or gzip file.\n","    \"\"\"\n","    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n","\n","    local_google_drive_path = os.path.join(data_path,file_path)\n","    local_file = Path(local_google_drive_path)\n","    if local_file.is_file():\n","        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n","    else:\n","        print(f\"Cannot find \\\"{file_path}\\\" so downloading it\")\n","        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n","        print(\"Done\")\n","\n","    if not gz_zip:\n","        f = open(local_google_drive_path, \"r\", encoding=\"utf-8-sig\")\n","    else:\n","        # Read compressed file (opened in text mode since that's what we use)\n","        f = gzip.open(local_google_drive_path, \"rt\", encoding=\"utf-8-sig\")\n","    results = [line.strip(\"\\n\") for line in f.readlines() if line]\n","\n","    if f:\n","      f.close()\n","\n","    return results\n","\n","\n","\n","# path to documents and stopwords\n","documents_path = \"PandP.gz\" # path to gzip file that contains documents\n","train_data_path = \"P1-train.gz\" # Path to p1 train dataset\n","stopwords_path = \"stopwords.txt\"  # path to the file that contains stopwords\n","\n","sentences = load_file(documents_path)\n","train_sentences = load_file(train_data_path)\n","stopwords = load_file(stopwords_path, gz_zip = False)\n","print(f\"First line of {documents_path:>16}: {sentences[0]}\");\n","print(f\"First line of {train_data_path:>16}: {train_sentences[0]}\");\n","print(f\"First line of {stopwords_path:>16}: {stopwords[0]}\");"]},{"cell_type":"markdown","metadata":{"id":"DCL5mLymTEPU"},"source":["### 1. Tokenization\n","\n","The first step of processing text is to break it into tokens. That will be either by using whitespace or two different fancier process. How you do that will be determined by the choice of tokenization function as follows.\n","\n","Each of these functions accepts a string (in this case, a line/sentence from one of the input files) and will produce a In either case, the tokens should preserve their order within original documents and if extra tokens are generated, they should be in the corresponding position of the token that triggered their creation."]},{"cell_type":"markdown","metadata":{"id":"ZUKVrr66TEPV"},"source":["#### 1.1 <u>tokenize_space()</u>\n","\n","This tokenizer will break the file into a list of whitespace-separated tokens. If there are multiple whitespace characters in a row (multiple spaces, a space and a tab, a space and a line break, etc.) they are treated as a single token break: e.g., \"a&lt;space&gt;&lt;space&gt;b\" is two tokens (\"a\" and \"b\") rather than three (\"a\", &lt;empty&gt;, \"b\"). Do not make any other changes to the tokens beyond separating them. So, for example, the start of this paragraph would result in the following 16 tokens (for the sake of space, listed across the line separated by ⍟, though that is not what you will actually do in your output):\n","\n","    This ⍟ tokenizer ⍟ will ⍟ break ⍟ the ⍟ file ⍟ into ⍟ a ⍟ list ⍟ of ⍟ whitespace-separated ⍟ tokens.\n","\n","Note that punctuation is included with a token based on where the whitespace is and tokens are produced without changing their case. There are very simple solutions to this: you can do this in one line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tt32gDp1TEPW"},"outputs":[],"source":["def tokenize_space(input_str: str) -> list[str]:\n","    \"\"\"\n","    Tokenize sentence into whitespace-separated tokens. For\n","    example, input_str = \"2024 Fall Search Engine\", the function\n","    should return [\"2024\", \"Fall\", \"Search\", \"Engine\"].\n","\n","    Args:\n","        input_str: the sentence/phrase that we want to tokenize.\n","\n","    Returns: an array of whitespace-separated tokens.\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    ans =input_str.split(' ')\n","    return ans# You will return something before this statement\n","\n","tokens_seperated_by_space = tokenize_space(sentences[0])\n","print(tokens_seperated_by_space)"]},{"cell_type":"markdown","source":["#### 1.2 tokenize_4grams()\n","\n","This is another straightforward tokenizer, but one that does have as many ready solutions in hand. Unlike the one above, this one treats _spaces as part of the token_. Every four characters is a token, regardless of whether the characters are whitespace, punctuation, or alphanumeric characters. The one exception so that is that you should treat any white-space type character (newline or tab) as if it were a space. _However_, if there are multiple whitespace characters in a row (multiple spaces, a space and a tab, a space and a line break, etc.) they are treated as a single space: e.g., \"a&lt;space&gt;&lt;space\\>bc\" is a single token (\"a&lt;space&gt;bc\") rather than the token \"a&lt;space&gt;&lt;space&gt;b\" and so on). Similarly, \"a&lt;space&gt;&lt;newline&gt;&lt;tab&gt;bcef\" should generate \"a&lt;space&gt;bc\" and \"bcef\".  \n","\n","It turns out that in most applications we need to use \"overlapping\" n-grams. So you should start each token two characters after the previous one. Using the same notation as above, the sequence\n","\n","    An  iced coffee \\t\n","    is very nice\n","\n","would generate the following sequence of tokens (using the same notation as above, with ⍟ showing where tokens break). Spaces are replaced with underscores so they are easier to see:\n","\n","    An_i ⍟ _ice ⍟ ced_ ⍟ d_co ⍟ coff ⍟ ffee ⍟ ee_i ⍟ _is_ ⍟ s_ve ⍟ very ⍟ ry_n ⍟ _nic ⍟ ice\n","\n","For the final token, do not include the 1-, 2-gram that ends that final token. And if the number of characters in the file is not a multiple of four, stop with the final 3-gram (like what we have in example).\n","\n","In special cases, if the string is too short to create a 4-gram, you should return the original token instead."],"metadata":{"id":"G0dJ6_27yacd"}},{"cell_type":"code","source":["def tokenize_4grams(input_str: str) -> list[str]:\n","    \"\"\"\n","    Tokenize sentence into 4char tokens with shifting windows. For example,\n","    input_str = \"An\\ticed coffee \\t\\nis very nice\", the function should return\n","    ['An i', ' ice', 'ced ', 'd co', 'coff', 'ffee', 'ee i', ' is ', 's ve', 'very', 'ry n', ' nic', 'ice'].\n","\n","    Args:\n","        input_str: the sentence/phrase that we want to tokenize.\n","\n","    Returns: an array of 4-grams tokens.\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    ans =[]\n","    for i in range(len(input_str)-3):\n","        ans.append(input_str[i:i+4])\n","    return []  # You will return something before this statement\n","\n","token_4grams = tokenize_4grams(sentences[0])\n","\n","# Check if the output matches the example mentioned above\n","print(' ⍟ '.join([elem.replace(' ', '_') for elem in tokenize_4grams(\"An\\ticed coffee \\t\\nis very nice\")]))"],"metadata":{"id":"oz_aNKqX0kJX","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1727382990243,"user_tz":240,"elapsed":313,"user":{"displayName":"Hung Nguyen","userId":"04521943953761589776"}},"outputId":"956b3a86-e2ba-4921-b7e2-286c6aa77037"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'sentences' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-592cef2bccac>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# You will return something before this statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtoken_4grams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_4grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check if the output matches the example mentioned above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"YinFt0HSTEPX"},"source":["#### 1.2 <u>tokenize_fancy()</u>\n","\n","This is a more complicated tokenizer that builds on the _spaces_ tokenizer (not the 4-gram version), applying the following additional rules. You can implement these rules in any way that makes sense to you as long as you honor the result of the rules as presented below, including dependencies between the rules. They are described on the assumption that you apply them as a sequence of steps in the order listed, though. Note that you will be returning the original token and its final version(s), so it might be helpful to think about how to maintain that information.\n","\n","The following tokenization rules has been listed in an order that we expect you to implement. *Hint: some of the steps generate new sequences that, in turn, need to be tokenized. You will probably want to think about a way to use recursion to handle that.*\n","\n","1. Start with **ONE** space-separated token as in the <em>spaces </em>tokenizer above.\n","\n","1. A token that is a URL of the form \"https://&hellip;\" or \"http://&hellip;\" should be recognized as a single token. If there is trailing punctuation, it is not part of the URL and that punctuation should be discarded. No other changes to a URL token should happen &ndash; i.e., for a URL, ignore the remaining rules below. Note that \"https://hithere.com+https://howdy.org\" should be considered as one simple token (since it starts with https). Do make sure to handle the cases of \"HTTP\" and \"HTTPS\" as the header of url is case-insensitive.\n","\n","3. Convert the tokens to lowercase.\n","  \n","4. Treat numbers as a single token, including plus and/or minus signs, commas, and decimal points if they exist, so that \"3,141.59\" remains that string. A number can only be a sequence of numeric characters and those punctuation marks indicated <span class=\"\" style=\"color: #98ca3e;\">but it must have at least one number character. You do not have to validate that the number is well formed (so \"3+14..1,59\" would still be a number token). Do no other processing to number tokens.\n","\n","5. Apostrophes should be \"squeezed out\" so that it is as if they were never there. That means that a contraction such as \"don't\" would turn into \"dont\" and a name like \"O'Brian\" will become \"obrian\" (because it is was also lowercased in step 3).\n","\n","6. Any remaining punctuation <u>other than periods and hyphens</u> should be treated as a word separator (as if it were white space: note the sequence of whitespace rules from the spaces tokenizer), except recall that punctuation in a URL or number is not a word break. So one token can generate multiple tokens: \"3/11//23\" will result in [\"3\", \"11\", \"23\"], and \"3^rd\" will result in [\"3\", \"rd\"], but \"3.14/pi\" will result in [\"3.14\", \"pi\"]. If a new token is a number (as defined above), do not process it further. So in the example, \"3.14\" will not be treated as an abbreviation by step 8. You may prefer to do this recursively to handle these rules on sub-parts. So that \"b.c.,\" (note the comma) would result in \"b.c.\" (remember that periods are not word breaks) and an empty token. A recursive call on \"b.c.\" will result in \"bc\" from abbreviation rule 8 and the empty token will be discarded.\n","\n","7. Hyphens (not treated as word separators) within a token should be processed in three steps: (1) remove the original token with its hyphen(s), (2) add new tokens with the hyphens treated as space separators (as in the spaces tokenizer), and then (3) add a new token with all hyphens squeezed out. So,\n","\n","  [\"data-base\"] &rarr; [\"data\", \"base\", \"database\"]<br/>\n","  [\"mother-in-law\"] &rarr; [\"mother\", \"in\", \"law\", \"motherinlaw\"]<br/>\n","  [\"a.-b.-c.\"] &rarr; [\"a.\", \"b.\", \"c.\", \"a.b.c.\"]\n","\n","  Note that as with the punctuation rule, the hyphen processing will result in additional tokens. Apply all tokenization rules to each of the tokens that are generated. So \"1.-on-1.\" will generate [\"1.\", \"on\", \"1.\", \"1.on.1.\"]. The occurrences of \"1.\" will remain unchanged because they are numbers, but \"1.on.1.\" will be treated as an abbreviation and converted into \"1on1\". There are some edge cases that will cause odd things to happen (e.g., the token \"1-https://Some.Url\" will end up converting the embedded URL to lowercase which contractadicts rule 2), but we won't stress about unusual subtokens caused by hyphens.\n","\n","8. Treat abbreviations as a single token. If a token is not a number or URL and the only punctuation it contains is periods, it is an abbreviation. Note that this includes a token that comprises nothing but periods, even if that is not intuitive. For an abbreviation, remove all periods. So \"i.b.m.\" converts to \"ibm\" and \"Ph.D.\" and \"Ph.D\" both go to \"phd\" (because an abbreviation does not need to end with a period and because rule 3 converted it to lowercase). If the result of removing the periods is an empty string, then treat it as an empty token and do not report it out."]},{"cell_type":"markdown","metadata":{"id":"DjVPROnPTEPY"},"source":["Here are some sample tokenizations that should happen:\n","* Token &rarr; token (lower case rule)\n","* She's &rarr; shes (apostrophe rule and lower case rule)\n","* Mother's-IN-Law &rarr; mothers, in, law, mothersinlaw (apostrophe, lowercase, hyphens)\n","* U.mass &rarr; umass (lowercase, abbreviation)\n","* go!!!!team &rarr; go team (non-period punctuation rule)\n","* USD\\$10.30 &rarr; usd 10.30 (non-period punctuation, case, number)\n","* USD\\$10,30 &rarr; usd 10 30 (not period punctuation, case)\n","* USD\\$10-30 &rarr; usd 10-30 (not-period punctuation, number (not hyphen!!))\n","\n","There will be interactions between rules, and that is OK. For example,\n","\n","* [\"a.-2./c.\"] &rarr; [\"a.-2.\", \"c.\"] by non-period and non-hyphen punctuation rule\n","* &rarr; [\"a.\", \"2.\", \"a.2.\", \"c.\"] by hyphen rule\n","* &rarr; [\"a\", \"2.\", \"a2\", \"c\"] after applying the abbreviation rule, noting that \"2.\" is a number so retains its decimal point.\n","\n","You may use a regular expression library to help identify patterns, but other third party libraries are _not_ allowed for this part. You may, of course, use the standard Python libraries."]},{"cell_type":"code","source":["def tokenize_fancy(input_token: str) -> list[str]:\n","    \"\"\"\n","    Tokenize ONE token into tokens using the fancy rules defined above.\n","\n","    Args:\n","        input_token: the token that we want to tokenize.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or\n","        [\n","            'c',\n","            'a',\n","            '2.',\n","            'a2'\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    inp =input_token.lower().split()\n","    return [] # You will return something before this statement\n","\n","\n","sample_sentence = 'a.-2./c. Ph.D. Ph. D. \\'\\'https://hithere.com+https://howdy.org!? https://noheading.com'\n","# Check if the output matches the example mentioned above\n","for token in tokenize_space(sample_sentence):\n","  tokens_from_fancy_tokenization = tokenize_fancy(token)\n","  print(token, tokens_from_fancy_tokenization)\n"],"metadata":{"id":"j-hfI34q7Qd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| a.-2./c. | ['c', 'a', '2.', 'a2'] |\n","| Ph.D. | ['phd' ] |\n","| Ph. | ['ph'] |\n","| D. | ['d'] |\n","| \\'\\'https://hithere.com+https://howdy.org!? | ['https', 'hitherecom', 'https', 'howdyorg'] |\n","| https://noheading.com | ['https://noheading.com'] |"],"metadata":{"id":"jCpetrP02vuC"}},{"cell_type":"markdown","metadata":{"id":"riW8cxuITEPZ"},"source":["### 2. Stopping\n","After tokenizing the input file, you will apply a stopword list or not:\n","\n","#### 2.1 <u>stopping()</u>\n","<u><strong>stopwords=None</strong></u>. means that you should not apply any stopping to the list. The result of this choice means that the token list will be unchanged by the stopping step.\n","\n","\n","<u><strong>stopwords=list[str]</strong></u>. means that you will use the list of words (that was loaded from the stopwords.txt file for you) as stopwords.\n","\n","It doesn't appy in your coding of this function, but note that if an original token results in multiple tokens (spaces or non-period punctuation rules), you must apply the stopword list to <em>each of</em> the resulting tokens.\n","\n","Also note that something that does not look like a stopword in the original text could become one &ndash; consider \"a-n\" in the text that will turn into [\"a\", \"n\", \"an\"] by the hyphen rule that will result in having \"a\" and \"an\" removed (if they're in the stopword list), but the \"n\" will be retained.\n","\n","Those issues should be taken care of in the higher-level user of this function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XI4L6TOHTEPZ"},"outputs":[],"source":["def stopping(input_tokens: list[str], stopwords: list[str] = None) -> list[str]:\n","    \"\"\"\n","    Applying stopping to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to apply stopwords.\n","        stopwords: the list of stopwords that need to be removed from list of token.\n","            Default to empty, i.e., do not stop.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or\n","        [\n","            \"a.-2./c.\",\n","            \"a\",\n","            \"2.\",\n","            \"c\",\n","            \"a2\",\n","            \"c\"\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    if stopwords == None:\n","      return input_tokens\n","    else:\n","      for token in input_tokens:\n","        if token in stopwords:\n","          input_tokens.remove(token)\n","      return input_tokens\n","     # You will return something before this statement\n","\n","\n","sample_sentence = 'a.-2./c. and hi'\n","for token in tokenize_space(sample_sentence):\n","    stop = stopping(\n","        tokenize_fancy(token), stopwords=stopwords\n","    )\n","    print(token, stop)"]},{"cell_type":"markdown","source":["Sample Result:\n","\n","| Phrase | Tokens | Token After Stopping |\n","| :------ | :------ | :------ |\n","| a.-2./c. | ['c', '2.', 'a', 'a2'] | ['c', '2.', 'a2'] |\n","| and | ['and'] | [] |\n","| hi | ['hi'] | ['hi'] |"],"metadata":{"id":"tSv5NBY54o4W"}},{"cell_type":"markdown","metadata":{"id":"rtaXXdooTEPZ"},"source":["### 3. Stemming\n","\n","After tokenizing the input file and possibly removing stopwords, you will stem each of the tokens in turn using one of these options. Note that stemming operates on every token that is not stopped, though in many cases (e.g., URLs, numbers, tokens that are already their stem) nothing will change. Also note that if a token generated multiple tokens (the spaces or non-period punctuation rules), the stemmer must be applied to each of them.\n","\n","#### 3.1 Stemming \"s\" suffix\n","<u><strong>stemming_type=None</strong></u>. means that you apply no stemming, so the token passes unchanged through this step.\n","\n","<u><strong>stemming_type=suffix_s</strong></u>. means that you apply the \"suffix-s\" stemmer, where if the final character of a token is \"s\" you remove the \"s\" and otherwise leave the token unchanged.\n","\n","(Porter comes next)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0znc5SECLUq"},"outputs":[],"source":["def stemming_s(input_tokens: list[str]) -> list[str]:\n","    \"\"\"\n","    Applying suffix-s stemming to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to perform stemming.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ]\n","\n","        For example, if the input is [\"hi\", \"is\", \"this\", \"lass\", \"silly\"] the output should be:\n","        [ \"hi\", \"i\", \"thi\", \"las\", \"silly\" ]\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    for token in input_tokens:\n","      if token[-1] == 's':\n","        input_tokens.remove(token)\n","        input_tokens.append(token[:-1])\n","    return [] # You will return something before this statement\n","\n","\n","sample_sentence = 'hi is this lass silly'\n","for token in tokenize_space(sample_sentence):\n","    stem = stemming_s(tokenize_fancy(token))  # Just tokenizes and skips stopping\n","    print(token, stem)"]},{"cell_type":"markdown","source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| hi | ['hi'] |\n","| is | ['i'] |\n","| this | ['thi'] |\n","| lass | ['las'] |\n","| silly | ['silly'] |"],"metadata":{"id":"aABOQHE07Kqj"}},{"cell_type":"markdown","source":["#### 3.2 Stemming with Porter Stemmer\n","\n","<u><strong>stemming_type=porter</strong></u>. means that you apply the Porter stemmer (we will use \"the English stemmer\" which is also called \"Porter2\"), though you will only implement part of the stemming algorithm. You'll find a description of the Porter2 stemmer in your textbook with Steps 1a and 1c listed. However, please use the following restatement of the algorithm where some of the ambiguities are clarified and where Step 1c is included.\n","\n","For each token in turn, apply the rule of Step 1a that matches the longest suffix (if it matches any) and then apply the one rule of Step 1b that matches the longest suffix (if any) of the output of Step 1a and then apply Step1c to Step 1b's result if it fits. In each step, if no suffix matches, do nothing and then continue to the next step.\n","\n","    Vowels are a, e, i, o, and u.\n","\n","As discussed in class (on Tuesday, September 17), the algorithm in the book talks about several things that are ill-defined as is. To address them, you need a method to determine a type of \"structure\" of the token being stemmed. Here that means figuring out how the token fits the pattern $[C](VC)^m[V]$ where $V$ is a sequence of one or more vowels (as above), C is a sequence of one or more consonents (anything that is not a vowel is a consonent), and $m$ indicates how often the $CV$ pattern repeats: if $m=0$ then it does not occur at all. Note that every word can be represented by this pattern. (Note that it's a weird concept for the 4-gram tokenizer. Treat the space as if it is a consonent.)\n","\n","Here are some examples to help you verify you're doing the right thing:\n","\n","* _agreed_ is _VCVC_ which is $(VC)^2$ with the leading $C$ and trailing $V$ empty.\n","* _retrieval_ is _CVCVCVC_ which is $C(VC)^3$ with the trailing $V$ empty.\n","* _feed_ is _CVC_ which is $C(VC)^1$ with the trailing $V$ empty.\n","* _tree_ is _CV_ which is $C(VC)^0V$ or $CV$ with nothing in the middle.\n","\n","And now, on to the steps of the algorithm, reworked slightly from the description in the book for clarity. Recall that each step is looking at the _end_ of the word.\n","\n","Step 1a:\n","  * Replace <em>sses</em> by <em>ss</em> (e.g., <em>stresses&rarr;stress</em>) and do nothing else for Step 1a.\n","  * If the stem ends with <em>ied</em> or <em>ies</em>, then if the remaining stem is more than one character long, replace the ending by _i_, otherwise replace it by _ie_ (e.g., <em>ties&rarr;tie, cries&rarr;cri</em>). In either case, do nothing else for Step 1a.\n","  * If the stem ends in <em>us </em>or <em>ss </em>do nothing (e.g., <em>stress&rarr;stress</em>), including doing nothing else for Step 1a\n","  * If the stem ends with _s_, then \"if the preceding stem part contains a vowel not immediately before the _s_\" then delete the trailing _s_  (e.g., <em>gaps&rarr;gap</em> but <em>gas&rarr;gas</em>). And do nothing else for Step 1a. (Note that the \"contains a vowel\" bit is not helped by the $m$ rule described above. You'll need another way to check for that.)\n","\n","Step 1b:\n","  * If the stem ends in <em>eed </em>or <em>eedly </em><u>then:</u>\n","      * if it is in the part of the stem after the first non-vowel following a vowel (i.e., if $m>1$ in the token that gets to 1b), replace it by <em>ee </em>(e.g., <em>agreed&rarr;agree, feed&rarr;feed</em>).\n","      * then go to step 1c\n","  * If the stem ends in <em>ed, edly, ing</em>, or <em>ingly </em>then:<u><strong> <br /></strong></u>\n","      * <u><strong></strong></u>if the preceding stem part <em>does not</em> contain a vowel, go to step 1c\n","\n","      * if the preceding stem part <em>does</em> contain a vowel delete the ending <u><em>and then also consider the following three possibilities with the resulting stem:</em></u>\n","          * if the stem now ends in <em>at</em>, <em>bl</em>, or <em>iz </em>add _e_ (e.g., <em>fished &rarr; fish, pirating &rarr;pirate</em>) and go to step 1c\n","\n","          * if the stem now ends with a double letter that is one of <em>bb, dd, ff, gg, mm, nn, pp, rr,</em> or <em>tt</em>, remove the last letter (e.g., <em>falling&rarr;fall, dripping&rarr;drip</em>) and go to step 1c\n","\n","          * if the stem is now short (defined as $m=1$ for the stem after removing the suffix), add _e_ (e.g., <em>hoping&rarr;hope</em>) and go to step 1c.\n","             * Some example short words by this definition are <em>at </em>or <em>ow </em>(or \"<em>or</em>\"!). Surprisingly, <em>be</em> is not short because it is $CV = C(VC)^0V$, so $m=0$ which is not 1.\n","             * These stems are also short: <em>shed, shred, rap, trap, hop</em>, or <em>bid </em>, yet these are not: <em>box</em>, <em>bow</em>, or <em>beds</em>).\n","\n","Step 1c (not in the textbook):\n","  * If the stem ends in <em>y </em>and the character before the <em>y </em>is neither a vowel nor the first letter of the word, replace the <em>y </em>with an <em>i </em>(e.g., <em>cry&rarr;cri, bi&rarr;bi, bambi&rarr;bamby, say&rarr;say</em>)\n","\n","Note: stemming_type matching should be case-insensitive, i.e., stemming_type=\"PoRtEr\" should also return the tokens after applying porter stemmer. Also, the default setting of <u>stemming()</u> should always be return the original tokens without stemming."],"metadata":{"id":"ooJF8HdrC-Ru"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8NuAW9GTEPZ"},"outputs":[],"source":["def stemming(input_tokens: list[str], stemming_type: str = None) -> list[str]:\n","    \"\"\"\n","    Applying stemming to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to apply stopping list.\n","        stemming_type: (case-insensitive) the stemming method that we want to apply on the token list.\n","              Options are [None, \"suffix_s\", \"porter\"].\n","              Default and unrecognized value(s) are considered as None.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or\n","        [\n","            \"a.-2./c.\",\n","            \"2.\",\n","            \"c\",\n","            \"a2\",\n","            \"c\"\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ## Make sure also include the suffix_s stemming function and return the results accordingly\n","    ##\n","    #########\n","    if stemming_type == \"suffix_s\":\n","      return []\n","    elif stemming_type == \"porter\":\n","      return []\n","    else:\n","\n","    return [] # You will return something before this statement\n","\n","\n","sample_sentence = 'a.-2./c. pirating gas hoping'\n","for token in tokenize_space(sample_sentence):\n","    stem = stemming_porter(\n","        stopping(tokenize_fancy(token), stopwords=stopwords),\n","        stemming_type=\"porter\"\n","    )\n","    print(token, stem)"]},{"cell_type":"markdown","source":["Sample Result:\n","\n","| Phrase | Tokens | Tokens After Stopping and Stemming |\n","| :------ | :------ | :------ |\n","| a.-2./c. | ['c', '2.', 'a', 'a2'] | ['c', '2.', 'a2'] |\n","| pirating | ['pirating'] | ['pirate'] |\n","| gas | ['gas'] | ['gas'] |\n","| hoping | ['hoping'] | ['hope'] |"],"metadata":{"id":"_vmaGZBM5KRL"}},{"cell_type":"markdown","metadata":{"id":"XYfjt1jDTEPZ"},"source":["### 4. Output & Statistics\n","\n","In the following, you will implement three function: <u>tokenization()</u>, <u>heaps()</u>, <u>statistics()</u>, which use the functions we just implemented."]},{"cell_type":"markdown","metadata":{"id":"UBdYEejrTEPZ"},"source":["#### 4.1 Tokenization with or without stopping and stemming: <u>tokenization()</u>\n","\n","<u>tokenization()</u> will tokenize a list of sentences given the option of your desire and return a list of processed token in the required format. Each element of returned list (a tuple) will contain the original space-separated word and a list of token that are generated using the original space-separated word. So if fancy tokenization, normal stopping, and the Porter stemming are enabled, the sentences\n","\n","    sentence 1: \"whitespace-Separated\" ⍟ tokens ⍟ (as ⍟ in ⍟ P0).\n","    sentence 2: And, ⍟ a.-2./c. ⍟ also.\n","    \n","where \"⍟\" indicates whitespace sequences, would result in\n","\n","| Phrase | Tokens|\n","| :------- | :-------- |\n","|<span class=\"\" style=\"color: #98ca3e;\">\"whitespace-Separated\"| [\"whitespace\", \"separate\", \"whitespaceseparate\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">tokens| [\"token\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">(as| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">in| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">P0).| [\"p0\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">And,| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">a.-2./c.| [\"2.\", \"c\", \"a2\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">also.| [\"also\"] |\n","\n","\n","Several expectation as follows:\n","* The tokens must be listed in the order that the original tokens were encountered in the input file.\n","* If an original token is removed (e.g., stopped) nothing will be printed for that token (e.g., the line for <span class=\"\" style=\"color: #98ca3e;\">\"(as\", <span class=\"\" style=\"color: #98ca3e;\">\"in\", and <span class=\"\" style=\"color: #98ca3e;\">\"And,\").\n","* If an original token is transformed into additional tokens (e.g., because of hyphens) all of the new tokens will be listed on the same line (e.g., the hyphenated word and the weird <span class=\"\" style=\"color: #98ca3e;\">\"a.-2./c.\" sequence).\n","* If a token is stemmed, the stem will be displayed with the original token. Note that the hyphenated word is separated, made into three tokens, and each of them is stemmed.\n","* Note that the process of tokenization with stopping and stemming can be executed as you read through the input file, though in this assignment we choose to read all the content in the file and then process it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJaSwEYJTEPb"},"outputs":[],"source":["def tokenization(\n","    input_list_str: list[str], stopwords: list[str] = None, stemming_type: str = None\n",") -> list[tuple[str, list[str]]]:\n","    \"\"\"\n","    Tokenize the input sentences and apply stopping and stemming procedure to the tokenized sentences.\n","\n","    Args:\n","        input_list_str: the list of sentences that we read from the input file.\n","        stopwords: the list of stopwords that need to be removed from list of token.\n","        stemming_type: (case-insensitive) the stemming method that we want to apply on the token list.\n","              Options are [None, \"suffix_s\", \"porter\"].\n","              Default and unrecognized value(s) are considered as None.\n","\n","    Returns: an array of processed tokens in required format, i.e.,\n","        [\n","            (\"\\\"whitespace-Separate\\\"\", [\"whitespace\", \"separate\", \"whitespaceseparate\"]),\n","            (\"Tokens\", [\"token\"]),\n","            ...\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    return []  # You will return something before this statement\n","\n","\n","sample_sentences = ['a.-2./c. pirating', 'gas hoping']\n","processed_tokens = tokenization(sample_sentences, stopwords=stopwords, stemming_type=\"porter\")\n","for phrase, tokens in processed_tokens:\n","    print(phrase, tokens)"]},{"cell_type":"markdown","source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| a.-2./c. | ['c', '2.', 'a2'] |\n","| pirating | ['pirate'] |\n","| gas | ['gas'] |\n","| hoping | ['hope'] |"],"metadata":{"id":"399jkoRn6jhM"}},{"cell_type":"markdown","metadata":{"id":"8gFi6e2RTEPb"},"source":["#### 4.2 Token Statistics - <u>heaps()</u>\n","<u>heaps()</u> analyze the processed tokens and count the number of valid tokens.\n","\n","Do not count stopwords if they are being removed, but do count the extra words that hyphens and other non-period punctuations generate. The return should have the format:[ (10, 8), (20, 17), ... ], where the first compoenent is the number of tokens you have generated so far (will be increasing multiples of 10, except possibly the very last one) and the second compoenent is the number of those tokens so far if you remove duplicates. For both counts, these are the tokens after the process of tokenizing, stopping, and stemming steps (indicated by the option in <u>tokenization()</u>), not before those steps. Clearly the second count will never be greater than the first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr30DxQfTEPb"},"outputs":[],"source":["def heaps(processed_tokens: list[tuple[str, list[str]]]) -> list[tuple[int, int]]:\n","    \"\"\"\n","    Analyze the processed tokens.\n","\n","    Args:\n","        processed_tokens: the list of processed tokens in the expected format.\n","\n","    Returns: an array of statistics in required format, i.e.,\n","        [\n","            (10, 8),\n","            (20, 17),\n","            ...\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    return []  # You will return something before this statement\n","\n","processed_tokens = tokenization(sentences, stopwords=stopwords, stemming_type=\"porter\")\n","heaps_result = heaps(processed_tokens)\n","[print(curr) for curr in heaps_result[:10]];"]},{"cell_type":"markdown","metadata":{"id":"QYzPzR-MTEPb"},"source":["#### 4.3 Token Statistics - <u>statistics()</u>\n","\n","<u>statistics()</u> analyze the statistic of the processed tokens and return the following information in this order:\n","    <ul>\n","        <li>A line containing the total number of tokens you encountered</li>\n","        <li>A line containing the total number of unique tokens you encountered</li>\n","        <li>An array contains the 100 most frequent final tokens you generated &ndash; i.e., after all of tokenizing, stopping, and stemming are done. Each element should have the format <code>[Token, Token_num]</code> where Token_num is the number of occurrences of token you found. <span class=\"\" style=\"color: #98ca3e;\">Sort them in descending order by the value of Token_num. If there are multiple words with the same value of Token_num, sort them alphabetically by the token. <span class=\"\" style=\"color: #98ca3e;\">If the same value of Token_num happens for the 100th and 101st items, just stop at 100.</li>\n","    </ul>\n","\n","Note that the first total number of tokens and unique tokens should match with the last element of the result of <u>heaps()</u>.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mWfpcVQTEPc"},"outputs":[],"source":["def statistics(\n","    processed_tokens: list[tuple[str, list[str]]]\n",") -> tuple[int, int, list[tuple[str, int]]]:\n","    \"\"\"\n","    Analyze the processed tokens.\n","\n","    Args:\n","        processed_tokens: the list of processed tokens in the expected format.\n","\n","    Returns:\n","        total number of tokens encountered;\n","        total number of unique tokens encountered;\n","        100 most frequent processed tokens encountered in required format, i.e.,\n","        [\n","            (\"Test\", 10),\n","            (\"Token\", 10),\n","            (\"Apple\", 9),\n","            ...\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    return []  # You will return something before this statement\n","\n","processed_tokens = tokenization(sentences, stopwords=stopwords, stemming_type=\"porter\")\n","stats_result = statistics(processed_tokens)\n","print(f\"Overall Token Counts: {stats_result[0]}; Overall Unique Token Counts: {stats_result[1]}\")\n","print(\"Top 10 frequent tokens with count:\")\n","[print(curr) for curr in stats_result[2][:10]];"]},{"cell_type":"markdown","metadata":{"id":"TbayZj2MTEPc"},"source":["### 5. Analysis questions"]},{"cell_type":"markdown","metadata":{"id":"EqNWkFobTEPc"},"source":["#### 5.1\n","Run your program with fancy tokenization, stopping, and Porter stemming on <em>PandP.gz</em> and look at the statistics() output to see the most frequent terms from _Pride and Prejudice_. Print the most frequent 100 words and their counts out, sorted in descending order by count). Break ties around the 100th word by alphabetical order of the tokens.\n","\n","After the list of terms, explain how the top terms are or are not relevant to the story or whether they seem like everyday words that aren't particularly to the novel? Support your answer with examples. You may find it useful to skim the <a class=\"\" href=\"https://en.wikipedia.org/wiki/Pride_and_Prejudice\">summary on Wikipedia</a> to know what words are part of the story."]},{"cell_type":"markdown","metadata":{"id":"CtavRPtPTEPc"},"source":["    Enter your answer here..."]},{"cell_type":"markdown","metadata":{"id":"nc37Mlf2TEPc"},"source":["#### 5.2\n","For this question, copy those same 100 words. Then say whether any of those top terms should have been stopwords (in your opinion)? Do the top terms or the list of all tokens suggest any changes to the tokenizing or stemming rules? What are they and why should they be made?"]},{"cell_type":"markdown","metadata":{"id":"2Ty8OFt9TEPc"},"source":["    Enter your answer here..."]},{"cell_type":"markdown","metadata":{"id":"Wl6LdWcWTEPd"},"source":["#### 5.3 Graph Comparison between result and Heaps' law - <u>graph_comparison()</u>\n","\n","Figure 4.4. in the textbook (p. 82) displays a graph of vocabulary growth for the TREC GOV2 collection. <u>graph_comparison()</u> should generate a similar graph which comparing the difference between the results of processed documents (from <u>heaps()</u>) and that of Heaps' Law. Try out some _K_ and _B_ values such that the produced line approximates the line comes from the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSQv9YyOTEPd","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"error","timestamp":1727676662980,"user_tz":240,"elapsed":173,"user":{"displayName":"Hung Nguyen","userId":"04521943953761589776"}},"outputId":"d206206a-a7ac-49c1-c8bf-ddc9b87f7210"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'heaps_result' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-89c2ae083a6d>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgraph_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-89c2ae083a6d>\u001b[0m in \u001b[0;36mgraph_comparison\u001b[0;34m(K, B)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mheaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaps_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'heaps_result' is not defined"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","def graph_comparison(K: float, B: float) -> None:\n","    \"\"\"\n","    Generates a graph that compares the results of processed documents and that of heaps' law.\n","    The curvature of heaps' law is computed using the function f(x) = K*x^B.\n","\n","    Args:\n","        K: heaps' law parameter,\n","        B: heaps' law parameter.\n","    \"\"\"\n","\n","    heaps = np.array(heaps_result)\n","\n","    X = heaps[:,0]\n","    dataset_y = heaps[:,1]\n","    heaps_y = [K * (elem ** B) for elem in X]\n","\n","    _, ax = plt.subplots(1)\n","    ax.plot(X, heaps_y, \"--\", label=f\"Heaps {K} {B}\")\n","    ax.plot(X, dataset_y, label=\"Dataset\")\n","    ax.set_title(\"Vocabulary growth for the Dataset\")\n","    ax.set_xlabel(\"Words in Collection\")\n","    ax.set_ylabel(\"Words in Vocabulary\")\n","    ax.set_xlim(xmin=0)\n","    ax.set_ylim(ymin=0)\n","    plt.legend()\n","    plt.show()\n","\n","\n","K = 40\n","B = 0.5\n","graph_comparison(K=K, B=B)"]},{"cell_type":"markdown","metadata":{"id":"nb8kY2mBTEPd"},"source":["(5.3 continued)\n","\n","Consider the graph generated above. Then say whether you feel that _Pride and Prejudice_ follows **Heaps' Law**? Why do you think so or think not? How did you decide on values of B and K?\n","\n","Note: when selecting pages for the question, make sure to include the graph along with your answer."]},{"cell_type":"markdown","metadata":{"id":"J8Jgvz5yTEPd"},"source":["    Enter your answer here\n"]},{"cell_type":"markdown","source":["### 6. Extra credit\n","\n","The following is an extra credit option for this assignment. Success on this part will possibly increase your score on P1. If you submit extra credit that does not work at all, you will lose one point on P1! **NOTE** We haven't figured out how much this is worth yet.\n","\n"],"metadata":{"id":"EIFTTw6AKpvd"}},{"cell_type":"code","source":["# Copy graph_comparison's code here to create one for zipf that plots the passed data and calculates the \"ideal\" Zipf's curve for comparison\n","\n","def zipf_graph_comparison(c: float = None) -> None:\n","    \"\"\"\n","    Generates a graph that compares the results of processed documents and that of zipf's law.\n","    The curvature of zipf's law is computed using the function f(x) = c/x, where x is the rank.\n","\n","    Args:\n","        c: zipf's law parameter. Using rank 1's probability if not specified.\n","    \"\"\"\n","\n","zipf_graph_comparison()"],"metadata":{"id":"Ypb7PkfkShPI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Consider the graph generated above. Then say whether you feel that _Pride and Prejudice_ follows **Zipf's law**? Why do you think so or think not? Discuss your idea with some reasoning.\n","\n","Note: when selecting pages for the question, make sure to include the graph along with your answer."],"metadata":{"id":"fvnNu9_1Sv91"}},{"cell_type":"markdown","source":["    Please enter your analysis of Zipf's Law here"],"metadata":{"id":"yfA0UGRcTGIU"}},{"cell_type":"markdown","metadata":{"id":"92M-bw8hTEPd"},"source":["### 7. Misc & Grading\n","\n","When you submit your code, the autograder will run:\n","\n","* up to 18 possible combinations of the command line on the training input (3 tokenizers, 2 stemmers + not stemming, stopping or not)\n","\n","* at least spaces-noStop-noStem and fancy-yesStop-porterStem on _Pride and Prejudice_\n","\n","* at least fancy-yesStop-porterStem on a held-out evaluation set that you do not have\n","\n","You may find a utility such as <em>diff</em> (Linux) helpful to compare your output to the expected output on <em>P1-train</em>. For the others, you'll have to eyeball it to see if it looks right. Or you may want to write a quick comparison function for your own use.\n","\n","Some things that you might want to consider while coding and debugging:\n","\n","* If you start with the <em>spaces</em> tokenizer and turn off stopping and stemming, you can check that this foundation step (breaking the text into space-separated tokens) works. That happens to be the same as the spaces-noStop-noStem sample output for P1-train.\n","\n","* Running the output of spaces though the stopping and stemming algorithms isn't likely to be very helpful for you (though we will grade it!) because the leftover punctuation reduces the number of times stemming occurs.\n","\n","* You can compare spaces-noStop-noStem to fancy-noStop-noStem to see if your fancier tokenizer is working. You can look to see that numbers and URLs pass through unchanged, the hyphens and other punctuation behave as described, and so on.\n","\n","* Then shift to fancy-yesStop-noStem to see if stopping is working\n","\n","* And then fancy-yesStop-yesStem for see if the whole process works"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.9 ('CS446')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a717fe07876ff9f5273f3c801eaea9bc6c04b7514a25233082091cb6ec1934fe"}},"colab":{"provenance":[{"file_id":"1LdSKzAXca6SlvPussAlN6c5e0bwEq06W","timestamp":1727382454070}]}},"nbformat":4,"nbformat_minor":0}